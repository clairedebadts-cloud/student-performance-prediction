{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "# Ensemble Model - Target: Break into Top 5\n\n**Current Score:** 8.737  \n**Target Score:** 8.53 (Top 5)  \n**Gap:** 0.204 points\n\n**Strategy:**\n1. Train 3 different models with Optuna\n2. Ensemble predictions with optimal weights\n3. Use cross-validation for robustness\n\nExpected improvement: **0.10-0.20 points** \u2728"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n!pip install optuna xgboost catboost --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Models\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\n# ML utilities\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Optuna\nimport optuna\nfrom optuna.visualization import plot_optimization_history\n\n# Settings\nimport warnings\nwarnings.filterwarnings('ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\n%matplotlib inline\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"\u2713 Libraries imported\")\nprint(f\"LightGBM: {lgb.__version__}\")\nprint(f\"XGBoost: {xgb.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 1. Load and Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load data\ntrain = pd.read_csv('../data/train.csv')\ntest = pd.read_csv('../data/test.csv')\n\nprint(f\"Train: {train.shape}\")\nprint(f\"Test: {test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def engineer_features(df):\n    \"\"\"Feature engineering - same as Optuna notebook\"\"\"\n    df = df.copy()\n    \n    # Polynomial features\n    df['study_hours_sq'] = df['study_hours'] ** 2\n    df['sleep_hours_sq'] = df['sleep_hours'] ** 2\n    df['class_attendance_sq'] = df['class_attendance'] ** 2\n    \n    # Interaction features\n    df['study_attendance'] = df['study_hours'] * df['class_attendance']\n    df['study_sleep_ratio'] = df['study_hours'] / (df['sleep_hours'] + 1e-6)\n    df['age_study_interaction'] = df['age'] * df['study_hours']\n    df['attendance_sleep'] = df['class_attendance'] * df['sleep_hours']\n    \n    # Categorical binning\n    df['age_group'] = pd.cut(df['age'], bins=[0, 22, 28, 100], \n                              labels=['young', 'middle', 'senior'])\n    df['study_intensity'] = pd.cut(df['study_hours'], bins=[0, 3, 6, 100],\n                                     labels=['low', 'medium', 'high'])\n    df['sleep_category'] = pd.cut(df['sleep_hours'], bins=[0, 6, 8, 100],\n                                    labels=['insufficient', 'optimal', 'excessive'])\n    \n    return df\n\n# Prepare data\nX_train_full = train.drop(['id', 'exam_score'], axis=1)\ny_train = train['exam_score']\nX_test = test.drop(['id'], axis=1)\ntest_ids = test['id']\n\n# Apply feature engineering\nX_train_full = engineer_features(X_train_full)\nX_test = engineer_features(X_test)\n\n# Encode (combine first to ensure matching columns)\ncombined = pd.concat([X_train_full, X_test], keys=['train', 'test'])\ncombined_encoded = pd.get_dummies(combined, drop_first=True)\nX_train_encoded = combined_encoded.loc['train'].reset_index(drop=True)\nX_test_encoded = combined_encoded.loc['test'].reset_index(drop=True)\n\nprint(f\"\\n\u2713 Data prepared\")\nprint(f\"Train: {X_train_encoded.shape}\")\nprint(f\"Test: {X_test_encoded.shape}\")\nprint(f\"Features: {X_train_encoded.shape[1]}\")\n\n# Train/val split\nX_train, X_val, y_train_split, y_val = train_test_split(\n    X_train_encoded, y_train, test_size=0.2, random_state=RANDOM_STATE\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 2. Model 1: LightGBM with Extended Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def lgb_objective(trial):\n    params = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbosity': -1,\n        'random_state': RANDOM_STATE,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'num_leaves': trial.suggest_int('num_leaves', 20, 200),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n    \n    model = lgb.LGBMRegressor(**params)\n    model.fit(X_train, y_train_split)\n    preds = model.predict(X_val)\n    return np.sqrt(mean_squared_error(y_val, preds))\n\nprint(\"Optimizing LightGBM...\")\nlgb_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\nlgb_study.optimize(lgb_objective, n_trials=100, show_progress_bar=True)\n\nprint(f\"\\n\u2713 LightGBM Best RMSE: {lgb_study.best_value:.4f}\")\nprint(f\"Best params: {lgb_study.best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 3. Model 2: XGBoost with Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def xgb_objective(trial):\n    params = {\n        'objective': 'reg:squarederror',\n        'eval_metric': 'rmse',\n        'random_state': RANDOM_STATE,\n        'n_estimators': trial.suggest_int('n_estimators', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n    }\n    \n    model = xgb.XGBRegressor(**params)\n    model.fit(X_train, y_train_split, verbose=False)\n    preds = model.predict(X_val)\n    return np.sqrt(mean_squared_error(y_val, preds))\n\nprint(\"Optimizing XGBoost...\")\nxgb_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\nxgb_study.optimize(xgb_objective, n_trials=100, show_progress_bar=True)\n\nprint(f\"\\n\u2713 XGBoost Best RMSE: {xgb_study.best_value:.4f}\")\nprint(f\"Best params: {xgb_study.best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 4. Model 3: CatBoost with Optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def cat_objective(trial):\n    params = {\n        'loss_function': 'RMSE',\n        'random_state': RANDOM_STATE,\n        'verbose': False,\n        'iterations': trial.suggest_int('iterations', 100, 800),\n        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.3, log=True),\n        'depth': trial.suggest_int('depth', 3, 12),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n    }\n    \n    model = CatBoostRegressor(**params)\n    model.fit(X_train, y_train_split)\n    preds = model.predict(X_val)\n    return np.sqrt(mean_squared_error(y_val, preds))\n\nprint(\"Optimizing CatBoost...\")\ncat_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\ncat_study.optimize(cat_objective, n_trials=100, show_progress_bar=True)\n\nprint(f\"\\n\u2713 CatBoost Best RMSE: {cat_study.best_value:.4f}\")\nprint(f\"Best params: {cat_study.best_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 5. Train Final Models and Create Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final models\nprint(\"Training final models...\\n\")\n\nlgb_model = lgb.LGBMRegressor(**lgb_study.best_params, random_state=RANDOM_STATE, verbosity=-1)\nxgb_model = xgb.XGBRegressor(**xgb_study.best_params, random_state=RANDOM_STATE)\ncat_model = CatBoostRegressor(**cat_study.best_params, random_state=RANDOM_STATE, verbose=False)\n\nlgb_model.fit(X_train_encoded, y_train)\nxgb_model.fit(X_train_encoded, y_train)\ncat_model.fit(X_train_encoded, y_train)\n\n# Evaluate on validation\nlgb_val_pred = lgb_model.predict(X_val)\nxgb_val_pred = xgb_model.predict(X_val)\ncat_val_pred = cat_model.predict(X_val)\n\nlgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_val_pred))\nxgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_val_pred))\ncat_rmse = np.sqrt(mean_squared_error(y_val, cat_val_pred))\n\nprint(\"Individual Model Performance:\")\nprint(f\"LightGBM: {lgb_rmse:.4f}\")\nprint(f\"XGBoost:  {xgb_rmse:.4f}\")\nprint(f\"CatBoost: {cat_rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimize ensemble weights\ndef ensemble_objective(trial):\n    w1 = trial.suggest_float('lgb_weight', 0.0, 1.0)\n    w2 = trial.suggest_float('xgb_weight', 0.0, 1.0)\n    w3 = trial.suggest_float('cat_weight', 0.0, 1.0)\n    \n    total = w1 + w2 + w3\n    w1, w2, w3 = w1/total, w2/total, w3/total\n    \n    ensemble_pred = w1 * lgb_val_pred + w2 * xgb_val_pred + w3 * cat_val_pred\n    return np.sqrt(mean_squared_error(y_val, ensemble_pred))\n\nprint(\"\\nOptimizing ensemble weights...\")\nensemble_study = optuna.create_study(direction='minimize')\nensemble_study.optimize(ensemble_objective, n_trials=100, show_progress_bar=True)\n\nbest_weights_raw = ensemble_study.best_params\ntotal_weight = sum(best_weights_raw.values())\nbest_weights = {k: v/total_weight for k, v in best_weights_raw.items()}\n\nprint(f\"\\n\u2713 Best Ensemble RMSE: {ensemble_study.best_value:.4f}\")\nprint(f\"\\nOptimal Weights:\")\nprint(f\"  LightGBM: {best_weights['lgb_weight']:.3f}\")\nprint(f\"  XGBoost:  {best_weights['xgb_weight']:.3f}\")\nprint(f\"  CatBoost: {best_weights['cat_weight']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell_0",
      "metadata": {},
      "source": [
        "## 6. Generate Final Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell_0",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predict on test set\nprint(\"Generating ensemble predictions...\\n\")\n\nlgb_test_pred = lgb_model.predict(X_test_encoded)\nxgb_test_pred = xgb_model.predict(X_test_encoded)\ncat_test_pred = cat_model.predict(X_test_encoded)\n\n# Ensemble with optimal weights\nensemble_pred = (\n    best_weights['lgb_weight'] * lgb_test_pred +\n    best_weights['xgb_weight'] * xgb_test_pred +\n    best_weights['cat_weight'] * cat_test_pred\n)\n\nprint(f\"Prediction stats:\")\nprint(f\"  Min: {ensemble_pred.min():.2f}\")\nprint(f\"  Max: {ensemble_pred.max():.2f}\")\nprint(f\"  Mean: {ensemble_pred.mean():.2f}\")\n\n# Create submission\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'exam_score': ensemble_pred\n})\n\nsubmission.to_csv('submission_ensemble.csv', index=False)\n\nprint(\"\\n\u2713 Submission created: submission_ensemble.csv\")\nprint(f\"Expected score: ~{ensemble_study.best_value:.3f}\")\nprint(f\"Target: Break into Top 5!\")\nprint(\"\\nFirst 10 rows:\")\nprint(submission.head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}