{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "m0",
   "metadata": {},
   "source": [
    "# Student Test Scores \u2014 Fixed & Optimised Ensemble\n\n## What was wrong before (and what this notebook fixes)\n\n| # | Bug | Fix |\n|---|-----|-----|\n| 1 | `GridSearchCV` trained on stale `X` (Cell 10), **missing all engineered features** | Feature-engineer \u2192 encode \u2192 split \u2192 train, in that order |\n| 2 | `internet_access`, `facility_rating`, `exam_difficulty` never encoded | Encode every categorical column |\n| 3 | Optimisation metric was MAE, competition metric is **RMSE** | Use `neg_root_mean_squared_error` everywhere |\n| 4 | `age_group` bins were arbitrary and inconsistent between notebooks | Data-driven quantile bins |\n| 5 | Train and test encoded separately \u2192 column mismatch | Concat \u2192 encode \u2192 split back |\n\n**Current leaderboard scores:**\n| Submission | Public RMSE |\n|---|---|\n| Submission 1 (single LGB) | 8.73744 |\n| Submission 2 (Optuna LGB) | 8.72869 |\n| Submission 3 (ensemble) | 8.72075 |\n| **Top 5** | **8.533\u20138.538** |\n| **Gap to close** | **\u2248 0.19** |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m1",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\n\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_squared_error\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nSEED = 42\nnp.random.seed(SEED)\nprint(\"\u2713 imports done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3",
   "metadata": {},
   "source": [
    "## 2. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\ntest  = pd.read_csv('../data/test.csv')\n\nprint(f\"train {train.shape}   test {test.shape}\")\nprint(\"\\ncolumns:\", train.columns.tolist())\nprint(\"\\ntarget stats:\\n\", train['exam_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m5",
   "metadata": {},
   "source": [
    "## 3. Quick EDA \u2014 understand the features before engineering them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# target distribution\naxes[0,0].hist(train['exam_score'], bins=60, color='steelblue', edgecolor='black')\naxes[0,0].set_title('Target: exam_score')\naxes[0,0].set_xlabel('Score')\n\n# numeric correlations with target\nnum_cols = train.select_dtypes(include='number').columns.tolist()\ncorr = train[num_cols].corr()['exam_score'].drop('exam_score').sort_values()\ncorr.plot(kind='barh', ax=axes[0,1], color='coral')\naxes[0,1].set_title('Numeric correlations with target')\n\n# categorical value counts\ncat_cols = ['gender','course','internet_access','sleep_quality',\n            'study_method','facility_rating','exam_difficulty']\nfor col in cat_cols:\n    print(f\"{col}: {train[col].nunique()} unique \u2192\", train[col].value_counts().to_dict())\n\n# mean target by each categorical (shows which have signal)\nmeans = {}\nfor col in cat_cols:\n    means[col] = train.groupby(col)['exam_score'].mean().std()  # std of group means = signal strength\nsignal = pd.Series(means).sort_values(ascending=False)\nsignal.plot(kind='barh', ax=axes[1,0], color='green')\naxes[1,0].set_title('Signal strength (std of group means)')\naxes[1,0].set_xlabel('Higher = more predictive')\n\n# age distribution\naxes[1,1].hist(train['age'], bins=30, color='purple', edgecolor='black')\naxes[1,1].set_title('Age distribution')\n\nplt.tight_layout()\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m7",
   "metadata": {},
   "source": [
    "## 4. Feature engineering\n\nRules:\n- **All** transformations go inside one function applied identically to train & test.\n- Categorical binning uses **data-driven** quantile cuts so they generalise.\n- We keep the raw features too \u2014 the tree models decide what to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer(df):\n    \"\"\"Deterministic feature engineering \u2014 call on train and test alike.\"\"\"\n    df = df.copy()\n\n    # \u2500\u2500 numeric interactions & polynomials \u2500\u2500\n    df['study_hours_sq']        = df['study_hours'] ** 2\n    df['sleep_hours_sq']        = df['sleep_hours'] ** 2\n    df['attendance_sq']         = df['class_attendance'] ** 2\n    df['study_x_attendance']    = df['study_hours'] * df['class_attendance']\n    df['study_sleep_ratio']     = df['study_hours'] / (df['sleep_hours'] + 1e-6)\n    df['age_x_study']           = df['age'] * df['study_hours']\n    df['attendance_x_sleep']    = df['class_attendance'] * df['sleep_hours']\n    df['study_x_sleep']         = df['study_hours'] * df['sleep_hours']\n    df['total_hours']           = df['study_hours'] + df['sleep_hours']\n    df['study_frac']            = df['study_hours'] / (df['total_hours'] + 1e-6)\n\n    # \u2500\u2500 quantile-based bins (edges fixed from TRAIN, applied to both) \u2500\u2500\n    # We pass edges explicitly so test uses the same boundaries\n    df['age_bin']      = pd.cut(df['age'],           bins=AGE_EDGES,      labels=False, include_lowest=True)\n    df['study_bin']    = pd.cut(df['study_hours'],   bins=STUDY_EDGES,    labels=False, include_lowest=True)\n    df['sleep_bin']    = pd.cut(df['sleep_hours'],   bins=SLEEP_EDGES,    labels=False, include_lowest=True)\n    df['attend_bin']   = pd.cut(df['class_attendance'], bins=ATTEND_EDGES, labels=False, include_lowest=True)\n\n    return df\n\n# Compute bin edges from TRAIN only (to avoid leakage)\ndef quantile_edges(series, n=4):\n    edges = series.quantile(np.linspace(0, 1, n + 1)).values\n    edges[0]  = -np.inf   # catch anything below\n    edges[-1] =  np.inf   # catch anything above\n    return edges\n\nAGE_EDGES     = quantile_edges(train['age'])\nSTUDY_EDGES   = quantile_edges(train['study_hours'])\nSLEEP_EDGES   = quantile_edges(train['sleep_hours'])\nATTEND_EDGES  = quantile_edges(train['class_attendance'])\n\nprint(\"Bin edges (train-derived):\")\nprint(f\"  age:        {AGE_EDGES}\")\nprint(f\"  study_hrs:  {STUDY_EDGES}\")\nprint(f\"  sleep_hrs:  {SLEEP_EDGES}\")\nprint(f\"  attendance: {ATTEND_EDGES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m9",
   "metadata": {},
   "source": [
    "## 5. Prepare train & test \u2014 engineer \u2192 encode \u2192 align\n\n**Key:** we concat train + test *before* `get_dummies` so every dummy column exists in both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and ids\ny_train  = train['exam_score']\nX_train  = train.drop(columns=['id', 'exam_score'])\nX_test   = test.drop(columns=['id'])\ntest_ids = test['id']\n\n# Engineer\nX_train = engineer(X_train)\nX_test  = engineer(X_test)\n\n# \u2500\u2500 encode together \u2500\u2500\ncombined         = pd.concat([X_train, X_test], keys=['train','test'])\ncombined_encoded = pd.get_dummies(combined, drop_first=True)\n\nX_train_enc = combined_encoded.loc['train'].reset_index(drop=True)\nX_test_enc  = combined_encoded.loc['test'].reset_index(drop=True)\n\n# sanity checks\nassert X_train_enc.shape[1] == X_test_enc.shape[1], \"column count mismatch!\"\nassert X_train_enc.isnull().sum().sum() == 0, \"NaN in train!\"\nassert X_test_enc.isnull().sum().sum() == 0,  \"NaN in test!\"\n\nprint(f\"\u2713 train {X_train_enc.shape}   test {X_test_enc.shape}\")\nprint(f\"  {X_train_enc.shape[1]} features total\")\nprint(f\"\\nFeatures:\\n{X_train_enc.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m11",
   "metadata": {},
   "source": [
    "## 6. 5-fold cross-validation helper\n\nUsing CV instead of a single train/val split gives a much more reliable\nestimate and prevents us from over-fitting to one particular split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "KF = KFold(n_splits=5, shuffle=True, random_state=SEED)\n\ndef cv_rmse(model_class, params, X, y):\n    \"\"\"Return mean CV RMSE for a given model + params.\"\"\"\n    scores = []\n    for tr_idx, val_idx in KF.split(X):\n        m = model_class(**params)\n        m.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n        pred = m.predict(X.iloc[val_idx])\n        scores.append(np.sqrt(mean_squared_error(y.iloc[val_idx], pred)))\n    return np.mean(scores), np.std(scores)\n\nprint(\"\u2713 CV helper ready (5-fold)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m13",
   "metadata": {},
   "source": [
    "## 7. Optuna \u2014 LightGBM (100 trials, 5-fold CV, RMSE metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_objective(trial):\n    params = dict(\n        objective      = 'regression',\n        metric         = 'rmse',          # \u2190 RMSE, not MAE\n        verbosity      = -1,\n        random_state   = SEED,\n        n_estimators   = trial.suggest_int('n_estimators', 200, 1000),\n        learning_rate  = trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n        max_depth      = trial.suggest_int('max_depth', 4, 14),\n        num_leaves     = trial.suggest_int('num_leaves', 24, 200),\n        min_child_samples = trial.suggest_int('min_child_samples', 10, 100),\n        subsample      = trial.suggest_float('subsample', 0.6, 1.0),\n        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        reg_alpha      = trial.suggest_float('reg_alpha', 1e-6, 5.0, log=True),\n        reg_lambda     = trial.suggest_float('reg_lambda', 1e-6, 5.0, log=True),\n    )\n    rmse, _ = cv_rmse(lgb.LGBMRegressor, params, X_train_enc, y_train)\n    return rmse\n\nlgb_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\nprint(\"Running LightGBM optimisation (100 trials \u00d7 5 folds)\u2026\")\nlgb_study.optimize(lgb_objective, n_trials=100, show_progress_bar=True)\nprint(f\"\\n\u2713 LightGBM  best CV RMSE: {lgb_study.best_value:.5f}\")\nprint(f\"  params: {lgb_study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m15",
   "metadata": {},
   "source": [
    "## 8. Optuna \u2014 XGBoost (100 trials, 5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_objective(trial):\n    params = dict(\n        objective      = 'reg:squarederror',\n        eval_metric    = 'rmse',\n        random_state   = SEED,\n        verbosity      = 0,\n        n_estimators   = trial.suggest_int('n_estimators', 200, 1000),\n        learning_rate  = trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n        max_depth      = trial.suggest_int('max_depth', 4, 14),\n        min_child_weight = trial.suggest_int('min_child_weight', 1, 10),\n        subsample      = trial.suggest_float('subsample', 0.6, 1.0),\n        colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        gamma          = trial.suggest_float('gamma', 1e-6, 1.0, log=True),\n        reg_alpha      = trial.suggest_float('reg_alpha', 1e-6, 5.0, log=True),\n        reg_lambda     = trial.suggest_float('reg_lambda', 1e-6, 5.0, log=True),\n    )\n    rmse, _ = cv_rmse(xgb.XGBRegressor, params, X_train_enc, y_train)\n    return rmse\n\nxgb_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\nprint(\"Running XGBoost optimisation (100 trials \u00d7 5 folds)\u2026\")\nxgb_study.optimize(xgb_objective, n_trials=100, show_progress_bar=True)\nprint(f\"\\n\u2713 XGBoost   best CV RMSE: {xgb_study.best_value:.5f}\")\nprint(f\"  params: {xgb_study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m17",
   "metadata": {},
   "source": [
    "## 9. Optuna \u2014 CatBoost (100 trials, 5-fold CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_objective(trial):\n    params = dict(\n        loss_function  = 'RMSE',\n        random_seed    = SEED,\n        verbose        = 0,\n        iterations     = trial.suggest_int('iterations', 200, 1000),\n        learning_rate  = trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n        depth          = trial.suggest_int('depth', 4, 12),\n        l2_leaf_reg    = trial.suggest_float('l2_leaf_reg', 1e-6, 5.0, log=True),\n        subsample      = trial.suggest_float('subsample', 0.6, 1.0),\n        colsample_bylevel = trial.suggest_float('colsample_bylevel', 0.6, 1.0),\n        min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 5, 100),\n    )\n    rmse, _ = cv_rmse(CatBoostRegressor, params, X_train_enc, y_train)\n    return rmse\n\ncat_study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=SEED))\nprint(\"Running CatBoost optimisation (100 trials \u00d7 5 folds)\u2026\")\ncat_study.optimize(cat_objective, n_trials=100, show_progress_bar=True)\nprint(f\"\\n\u2713 CatBoost  best CV RMSE: {cat_study.best_value:.5f}\")\nprint(f\"  params: {cat_study.best_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m19",
   "metadata": {},
   "source": [
    "## 10. Out-of-fold predictions \u2192 optimise blend weights\n\nInstead of a single train/val split we collect **out-of-fold (OOF)** predictions\nfor every row in the training set. This gives an unbiased estimate of each\nmodel's predictions and lets us find the best blend weights reliably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_predict(model_class, params, X, y, X_test):\n    \"\"\"Return OOF predictions on X and averaged test predictions.\"\"\"\n    oof  = np.zeros(len(X))\n    test_preds = np.zeros(len(X_test))\n\n    for tr_idx, val_idx in KF.split(X):\n        m = model_class(**params)\n        m.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n        oof[val_idx]  = m.predict(X.iloc[val_idx])\n        test_preds   += m.predict(X_test) / KF.get_n_splits()\n\n    return oof, test_preds\n\nprint(\"Generating OOF + test predictions for each model\u2026\\n\")\n\nlgb_oof, lgb_test = oof_predict(lgb.LGBMRegressor,\n    {**lgb_study.best_params, 'verbosity': -1, 'random_state': SEED},\n    X_train_enc, y_train, X_test_enc)\nprint(f\"  LGB OOF RMSE: {np.sqrt(mean_squared_error(y_train, lgb_oof)):.5f}\")\n\nxgb_oof, xgb_test = oof_predict(xgb.XGBRegressor,\n    {**xgb_study.best_params, 'verbosity': 0, 'random_state': SEED},\n    X_train_enc, y_train, X_test_enc)\nprint(f\"  XGB OOF RMSE: {np.sqrt(mean_squared_error(y_train, xgb_oof)):.5f}\")\n\ncat_oof, cat_test = oof_predict(CatBoostRegressor,\n    {**cat_study.best_params, 'verbose': 0, 'random_seed': SEED},\n    X_train_enc, y_train, X_test_enc)\nprint(f\"  CAT OOF RMSE: {np.sqrt(mean_squared_error(y_train, cat_oof)):.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m21",
   "metadata": {},
   "source": [
    "## 11. Optimise blend weights on OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blend_objective(trial):\n    w_lgb = trial.suggest_float('w_lgb', 0.0, 1.0)\n    w_xgb = trial.suggest_float('w_xgb', 0.0, 1.0)\n    w_cat = trial.suggest_float('w_cat', 0.0, 1.0)\n    s = w_lgb + w_xgb + w_cat\n    w_lgb, w_xgb, w_cat = w_lgb/s, w_xgb/s, w_cat/s\n\n    blend = w_lgb * lgb_oof + w_xgb * xgb_oof + w_cat * cat_oof\n    return np.sqrt(mean_squared_error(y_train, blend))\n\nblend_study = optuna.create_study(direction='minimize')\nblend_study.optimize(blend_objective, n_trials=200)\n\nraw = blend_study.best_params\ns   = sum(raw.values())\nW   = {k: v/s for k, v in raw.items()}\n\nprint(f\"\\n\u2713 Optimal blend weights:\")\nprint(f\"    LightGBM : {W['w_lgb']:.3f}\")\nprint(f\"    XGBoost  : {W['w_xgb']:.3f}\")\nprint(f\"    CatBoost : {W['w_cat']:.3f}\")\nprint(f\"  Blend OOF RMSE: {blend_study.best_value:.5f}\")\nprint(f\"\\n  vs best single model OOF RMSE: {min(np.sqrt(mean_squared_error(y_train, lgb_oof)), np.sqrt(mean_squared_error(y_train, xgb_oof)), np.sqrt(mean_squared_error(y_train, cat_oof))):.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m23",
   "metadata": {},
   "source": [
    "## 12. Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blend test predictions with optimised weights\nfinal_pred = W['w_lgb'] * lgb_test + W['w_xgb'] * xgb_test + W['w_cat'] * cat_test\n\n# Clip to valid range (exam scores are 0-100)\nfinal_pred = np.clip(final_pred, 0, 100)\n\nsubmission = pd.DataFrame({'id': test_ids, 'exam_score': final_pred})\nsubmission.to_csv('submission_v4.csv', index=False)\n\nprint(\"=\"*60)\nprint(\"SUBMISSION READY: submission_v4.csv\")\nprint(\"=\"*60)\nprint(f\"\\nRows: {len(submission)}\")\nprint(f\"Pred range: [{final_pred.min():.2f}, {final_pred.max():.2f}]\")\nprint(f\"Pred mean:  {final_pred.mean():.2f}\")\nprint(f\"\\nOOF RMSE estimates:\")\nprint(f\"  LightGBM : {np.sqrt(mean_squared_error(y_train, lgb_oof)):.5f}\")\nprint(f\"  XGBoost  : {np.sqrt(mean_squared_error(y_train, xgb_oof)):.5f}\")\nprint(f\"  CatBoost : {np.sqrt(mean_squared_error(y_train, cat_oof)):.5f}\")\nprint(f\"  Blend    : {blend_study.best_value:.5f}\")\nprint(\"\\n\" + submission.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m25",
   "metadata": {},
   "source": [
    "## 13. Feature importance (averaged across folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train one LGB on full data just to visualise importances\nlgb_full = lgb.LGBMRegressor(**lgb_study.best_params, verbosity=-1, random_state=SEED)\nlgb_full.fit(X_train_enc, y_train)\n\nimp = pd.DataFrame({'feature': X_train_enc.columns,\n                     'importance': lgb_full.feature_importances_}\n                   ).sort_values('importance', ascending=False)\n\nplt.figure(figsize=(10, 8))\nplt.barh(imp['feature'][:20][::-1], imp['importance'][:20][::-1], color='steelblue')\nplt.xlabel('Importance')\nplt.title('Top 20 Feature Importances (LightGBM)')\nplt.tight_layout()\nplt.show()\n\nprint(imp.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}